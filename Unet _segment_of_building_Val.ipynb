{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import re\n",
    "import hickle as hkl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#from torch.legacy.nn import Reshape\n",
    "import graphviz\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "#from visualize import make_dot\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "import scipy\n",
    "import time\n",
    "import logging\n",
    "import cv2\n",
    "from math import log,sqrt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from misc import initialize_weights1\n",
    "\n",
    "\n",
    "class _EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout=False):\n",
    "        super(_EncoderBlock, self).__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout())\n",
    "        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.encode = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encode(x)\n",
    "\n",
    "\n",
    "class _DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super(_DecoderBlock, self).__init__()\n",
    "        self.decode = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, middle_channels, kernel_size=3),\n",
    "            nn.BatchNorm2d(middle_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(middle_channels, middle_channels, kernel_size=3),\n",
    "            nn.BatchNorm2d(middle_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decode(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.enc1 = _EncoderBlock(3, 64)\n",
    "        self.enc2 = _EncoderBlock(64, 128)\n",
    "        self.enc3 = _EncoderBlock(128, 256)\n",
    "        self.enc4 = _EncoderBlock(256, 512, dropout=True)\n",
    "        self.center = _DecoderBlock(512, 1024, 512)\n",
    "        self.dec4 = _DecoderBlock(1024, 512, 256)\n",
    "        self.dec3 = _DecoderBlock(512, 256, 128)\n",
    "        self.dec2 = _DecoderBlock(256, 128, 64)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.final = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "        initialize_weights1(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(enc1)\n",
    "        enc3 = self.enc3(enc2)\n",
    "        enc4 = self.enc4(enc3)\n",
    "        center = self.center(enc4)\n",
    "        dec4 = self.dec4(torch.cat([center, F.upsample(enc4, center.size()[2:], mode='bilinear')], 1))\n",
    "        dec3 = self.dec3(torch.cat([dec4, F.upsample(enc3, dec4.size()[2:], mode='bilinear')], 1))\n",
    "        dec2 = self.dec2(torch.cat([dec3, F.upsample(enc2, dec3.size()[2:], mode='bilinear')], 1))\n",
    "        dec1 = self.dec1(torch.cat([dec2, F.upsample(enc1, dec2.size()[2:], mode='bilinear')], 1))\n",
    "        final = self.final(dec1)\n",
    "        x=F.upsample(final, x.size()[2:], mode='bilinear')\n",
    "        x = x.view(-1,512,512)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Build_Segmentation_new\\misc.py:20: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(module.weight)\n"
     ]
    }
   ],
   "source": [
    "net = UNet(1)\n",
    "#print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input=Variable(torch.randn(1,3,512,512))\n",
    "output=net(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[:,1:5,1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File('DM_TrainDataset.h5')\n",
    "xtrainT = torch.from_numpy(np.array(file['xtrain'],dtype=np.float32)).float()\n",
    "ytrainT = torch.from_numpy(np.array(file['ytrain'],dtype=np.float32)).float()\n",
    "#xtrain = np.array(file['xtrain'],dtype=np.float32)\n",
    "#ytrain = np.array(file['ytrain'],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File('DM_TestDataset.h5')\n",
    "xtestT = torch.from_numpy(np.array(file['xtest'],dtype=np.float32)).float()\n",
    "ytestT = torch.from_numpy(np.array(file['ytest'],dtype=np.float32)).float()\n",
    "#xtest = np.array(file['xtest'],dtype=np.float32)\n",
    "#ytest = np.array(file['ytest'],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_rgb_to_bgr(batch):\n",
    "    #print(batch.size())\n",
    "    (r, g, b) = torch.chunk(batch, 3, 1)\n",
    "    #print(r.size())\n",
    "    batch1 = torch.cat((b, g, r),1)\n",
    "    #print(batch1.size())\n",
    "    return batch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xtrainT = batch_rgb_to_bgr(xtrainT)\n",
    "print(xtrainT.size(),ytrainT.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrainT = torch.div(xtrainT,255.0)\n",
    "ytrainT = torch.div(ytrainT,255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([249, 3, 512, 512]) torch.Size([249, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "xtestT = batch_rgb_to_bgr(xtestT)\n",
    "\n",
    "print(xtestT.size(),ytestT.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtestT = torch.div(xtestT,255.0)\n",
    "ytestT = torch.div(ytestT,255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(torch.min(xtrainT), torch.max(xtrainT), torch.min(xtestT), torch.max(xtestT))\n",
    "print(torch.min(xtrainT), torch.max(xtrainT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xtrainT.size(), ytrainT.size(), xtestT.size(), ytestT.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    \"\"\"\n",
    "    Normalize an tensor image with mean and standard deviation.\n",
    "    Given mean: (R, G, B) and std: (R, G, B),\n",
    "    will normalize each channel of the torch.*Tensor, i.e.\n",
    "    channel = (channel - mean) / std\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for R, G, B channels respecitvely.\n",
    "        std (sequence): Sequence of standard deviations for R, G, B channels\n",
    "            respecitvely.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        # TODO: make efficient\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.sub_(m).div_(s)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xtrainT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-8b14bea9699e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.225\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.224\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.229\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mxtrainT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrainT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'xtrainT' is not defined"
     ]
    }
   ],
   "source": [
    "mn = [0.406,0.456,0.485]\n",
    "sd = [0.225,0.224,0.229]\n",
    "norm = Normalize(mn,sd)\n",
    "xtrainT = norm(xtrainT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = [0.406,0.456,0.485]\n",
    "sd = [0.225,0.224,0.229]\n",
    "norm = Normalize(mn,sd)\n",
    "xtestT = norm(xtestT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.min(xtrainT), torch.max(xtrainT), torch.min(xtestT), torch.max(xtestT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrainT=xtestT\n",
    "ytrainT=ytestT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtestT = batch_rgb_to_bgr(xtestT)\n",
    "#xtestT = torch.div(xtestT,255.0)\n",
    "#mn = [0.406,0.456,0.485]\n",
    "#sd = [0.225,0.224,0.229]\n",
    "#norm = Normalize(mn,sd)\n",
    "#xtestT = norm(xtestT)\n",
    "#print(xtestT.size(), ytestT.size())\n",
    "#print(torch.min(xtestT), torch.max(xtestT),torch.min(ytestT), torch.max(ytestT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##def train(model, loss, optimizer, x_val, y_val, validPixel, batch_sz):\n",
    "def train(model, loss, optimizer, x_val, y_val,batch_size):\n",
    "    x = Variable(x_val,requires_grad = False).cuda()\n",
    "    y = Variable(y_val,requires_grad = False).cuda()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    fx = model.forward(x)\n",
    "    \n",
    "    #print fx.data[0][0][64][87]\n",
    "    #fx = model5.forward(Variable(xtest2[start:end], volatile=True).cuda())\n",
    "    ##output = loss.forward(fx,y,validPixel,batch_sz)\n",
    "    output = loss.forward(fx,y)\n",
    "    #output = loss(fx, y)\n",
    "    output.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##def train(model, loss, optimizer, x_val, y_val, validPixel, batch_sz):\n",
    "def test(model, loss, x_val, y_val):\n",
    "    x = Variable(x_val,requires_grad = False).cuda()\n",
    "    y = Variable(y_val,requires_grad = False).cuda()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    fx = model.forward(x)\n",
    "    \n",
    "    #print fx.data[0][0][64][87]\n",
    "    #fx = model5.forward(Variable(xtest2[start:end], volatile=True).cuda())\n",
    "    ##output = loss.forward(fx,y,validPixel,batch_sz)\n",
    "    output = loss.forward(fx,y)\n",
    "    #output = loss(fx, y)\n",
    "   \n",
    "    \n",
    "    return output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom loss function.... this will be reverse Huber...\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        \n",
    "    def forward(self,inp, tar):\n",
    "        #target is the ground truth value...\n",
    "        #k = torch.mean(inp[:,0])\n",
    "        '''\n",
    "        if (k >= 1.48 and k <= 1.65):\n",
    "            diff = torch.abs(tar[:,1]-inp[:,1])\n",
    "            loss = torch.mean(torch.pow(diff,2))\n",
    "        else:\n",
    "        '''\n",
    "#         diff1 = (tar[:,0]-inp[:,0]) #*(180/np.pi)\n",
    "#         diff=torch.pow(diff,2)\n",
    "#         loss = torch.mean(diff)\n",
    "        loss = torch.sqrt( torch.mean( torch.abs(torch.log(tar)-torch.log(inp)) ** 2 ) )\n",
    "        #print(loss)\n",
    "        return loss\n",
    "        '''\n",
    "        c1 = c.data[0] \n",
    "        temp = diff > c1\n",
    "        check1 = torch.prod(temp)\n",
    "        \n",
    "        if check1 == 0:\n",
    "            lossval = torch.mean(diff)\n",
    "        else:\n",
    "            temp4 = torch.pow(diff,2)\n",
    "            d = torch.pow(c,2)\n",
    "            temp4 = temp4.add(d.expand_as(temp4))\n",
    "            lossval = torch.mean(temp4/(2*c))\n",
    "        return lossval\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom loss function....\n",
    "\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        \n",
    "    def forward(self,input, target):\n",
    "        diff = torch.abs(target-input)\n",
    "        temp1 = diff**2\n",
    "        temp2 = torch.mean(temp1)\n",
    "        return torch.sqrt(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dice_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dice_loss, self).__init__()\n",
    "    def forward(self,input, target):\n",
    "        smooth = 1.\n",
    "        input = input[:,1,None].sigmoid()\n",
    "        iflat = input.contiguous().view(-1).float()\n",
    "        tflat = target.view(-1).float()\n",
    "        intersection = (iflat * tflat).sum()\n",
    "        return (1 - ((2. * intersection + smooth) / ((iflat + tflat).sum() +smooth))) \n",
    "#     def forward(self,input, target):\n",
    "#         bce_loss = CrossEntropyFlat(axis=1)\n",
    "#         return bce_loss(pred,targ) + dice_loss(pred,targ)\n",
    "        \n",
    "#     def forward(self,input, target):\n",
    "#         diff = torch.abs(target-input)\n",
    "#         temp1 = diff**2\n",
    "#         temp2 = torch.mean(temp1)\n",
    "#         return torch.sqrt(temp2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# def dice_loss(input, target):\n",
    "# #     pdb.set_trace()\n",
    "#     smooth = 1.\n",
    "#     input = input[:,1,None].sigmoid()\n",
    "#     iflat = input.contiguous().view(-1).float()\n",
    "#     tflat = target.view(-1).float()\n",
    "#     intersection = (iflat * tflat).sum()\n",
    "#     return (1 - ((2. * intersection + smooth) / ((iflat + tflat).sum() +smooth)))\n",
    "\n",
    "# def combo_loss(pred, targ):\n",
    "#     bce_loss = CrossEntropyFlat(axis=1)\n",
    "#     return bce_loss(pred,targ) + dice_loss(pred,targ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogLoss, self).__init__()\n",
    "        \n",
    "    def forward(self,inp, tar):\n",
    "     \n",
    "        loss = torch.sqrt( torch.mean( torch.abs(torch.log(tar)-torch.log(inp)) ** 2 ) )\n",
    "        #print(loss)\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class BerhuLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BerhuLoss, self).__init__()\n",
    "        \n",
    "    def forward(self,inp, tar):\n",
    "        #target is the ground truth value...\n",
    "        mt = tar[:,0]\n",
    "        mp = inp[:,0]\n",
    "        diff = torch.abs(mt-mp)        \n",
    "        lossval = 0.0        \n",
    "        c = 0.2 * torch.max(diff)\n",
    "        l1 = torch.mean(diff)\n",
    "        l2 = torch.mean(torch.pow(diff,2))\n",
    "        if l1 <= c:\n",
    "            lossval = l1\n",
    "        else:\n",
    "            lossval = (l2+c**2)/(2*c)\n",
    "        \n",
    "        return lossval\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha = torch.FloatTensor(ytrainT[5,0])\n",
    "alpha = ytrainT[5,0]\n",
    "#print(alpha.shape)\n",
    "xt = torch.FloatTensor([np.cos(alpha),np.sin(alpha)])\n",
    "print(ytrainT[5,0],xt.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = ytrainT[5:10,0]\n",
    "print(torch.cos(alpha[0:1]-alpha[1:2]))\n",
    "xt = torch.stack([torch.cos(alpha[0:1]),torch.sin(alpha[0:1])])\n",
    "xp = torch.stack([torch.cos(alpha[1:2]),torch.sin(alpha[1:2])])\n",
    "print(xt[0],xt[1])\n",
    "#print(los)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CosineLoss, self).__init__()\n",
    "        \n",
    "    def forward(self,inp, tar,batch_sz):\n",
    "        alpha_t = tar[:,0]\n",
    "        alpha_p = inp[:,0]\n",
    "        #xt = torch.stack([torch.cos(alpha_t),torch.sin(alpha_t)])\n",
    "        #xp = torch.stack([torch.cos(alpha_p),torch.sin(alpha_p)])\n",
    "        #cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        #loss = cos(xt, xp)\n",
    "        #return loss\n",
    "        loss = Variable(torch.FloatTensor(batch_sz).zero_(), requires_grad=False).cuda()\n",
    "        for i in range(batch_sz):          \n",
    "            loss[i] = torch.cos(alpha_t[i:i+1]-alpha_p[i:i+1])\n",
    "            \n",
    "        lossval = 1.0-torch.mean(loss)    \n",
    "        #print(lossval)\n",
    "        return lossval\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save the weights\n",
      "Loss = 0.33649849 val Loss = 0.33717404 at epoch 1  completed in 0m 46s\n",
      "Loss 0.39132429994493123 is bigger than Loss 0.3371740415034045 in the prev epoch \n",
      "Loss = 0.33649849 val Loss = 0.39132430 at epoch 2  completed in 0m 46s\n",
      "Loss 0.36346000247451676 is bigger than Loss 0.3371740415034045 in the prev epoch \n",
      "Loss = 0.33649849 val Loss = 0.36346000 at epoch 3  completed in 0m 46s\n",
      "Loss 0.3544740830080576 is bigger than Loss 0.3371740415034045 in the prev epoch \n",
      "Loss = 0.33649849 val Loss = 0.35447408 at epoch 4  completed in 0m 46s\n",
      "save the weights\n",
      "Loss = 0.33649849 val Loss = 0.33609353 at epoch 5  completed in 0m 46s\n"
     ]
    }
   ],
   "source": [
    "#MUST UNCOMMENT BELOW LINE...\n",
    "    \n",
    "net = net.cuda()\n",
    "\n",
    "#loading the model after the weights of epoch50.. to check what loss the model gives if lr is taken as 0.0001\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "#criterion = RMSELoss()\n",
    "#criterion = BerhuLoss()\n",
    "#criterion = EuclideanLoss()\n",
    "#criterion = nn.MSELoss()\n",
    "#criterion = CosineLoss()\n",
    "#criterion = torch.nn.MSELoss(size_average=False)\n",
    "#criterion = CustomLoss()\n",
    "#criterion = LogLoss()\n",
    "criterion = RMSELoss()\n",
    "#criterion = Dice_loss()\n",
    "#criterion = BerhuLoss()\n",
    "#criterion =  CosineLoss()\n",
    "#criterion = nn.MSELoss()\n",
    "#criterion = nn.LogLoss()\n",
    "criterion.cuda()\n",
    "\n",
    "val_currepochloss =float('Inf')\n",
    "#epochs, n_examples, i, batch_size, flag = 1,5900, 0, 5, 0\n",
    "epochs, n_examples, i, batch_size, flag = 5, 249, 0, 5, 0\n",
    "\n",
    "mean_val_losses = []\n",
    "while i != epochs:\n",
    "    val_losses = []\n",
    "    since = time.time()\n",
    "    cost, batchloss = 0.0, 0.0\n",
    "    num_batches = n_examples//batch_size\n",
    "    #print num_batches    #indices = np.random.permutation(5600)\n",
    "    #indices = np.random.permutation(3524)\n",
    "    \n",
    "    #indices = np.random.permutation(5900)\n",
    "    indices = np.random.permutation(n_examples)\n",
    "    samplesUnprocessed = np.size(indices)\n",
    "    \n",
    "    #batchwise training starts here...\n",
    "    for k in range(num_batches):\n",
    "        since1 = time.time()\n",
    "       # print(\"bacth number:\"+str(k))\n",
    "        xtrain3 = torch.FloatTensor(batch_size,3,512,512)\n",
    "        ytrain3 = torch.FloatTensor(batch_size,512,512)\n",
    "        ##validPixel = torch.FloatTensor(batch_size,480,640)\n",
    "        \n",
    "        for ind in range(batch_size):\n",
    "            #ind1 = np.random.randint(0,5599)\n",
    "            ind1 = np.random.randint(0,samplesUnprocessed)\n",
    "            #ind1 = np.random.randint(0,794)\n",
    "            #ind1 = np.random.randint(0,794)            \n",
    "            newxind = indices[ind1]            \n",
    "            xtrain3[ind] = xtrainT[newxind]\n",
    "            ytrain3[ind] = ytrainT[newxind]\n",
    "            ##validPixel[ind] = imgValidTrain2[newxind]\n",
    "            \n",
    "            #print ytrain3[ind,0,0,0], ytrain2[newxind,0,0,0]\n",
    "            indices = np.delete(indices,ind1)\n",
    "            samplesUnprocessed = samplesUnprocessed - 1\n",
    "        \n",
    "        #start, end = k*batch_size, (k+1)*batch_size\n",
    "        #batchloss = train(model5,criterion, optimizer, xtrain3, ytrain3, validPixel,batch_size)\n",
    "        batchloss = train(net,criterion, optimizer, xtrain3, ytrain3, batch_size)\n",
    "        batch_time = time.time() - since1\n",
    "        #cost += batchloss\n",
    "        cost = (cost*k+batchloss)/(k+1)\n",
    "        #print k,cost\n",
    "        #print(\"No. of samples UnProcessed \"+str(samplesUnprocessed))\n",
    "        \n",
    "    for k in range(xtestT.size()[0]):\n",
    "        since1 = time.time()\n",
    "        xtest3 = torch.FloatTensor(1,3,512,512)\n",
    "        ytest3 = torch.FloatTensor(1,512,512)\n",
    "        xtest3[0] = xtrainT[k]\n",
    "        ytest3[0] = ytrainT[k]\n",
    "        Val_loss = test(net,criterion, xtest3, ytest3)\n",
    "        val_losses.append(Val_loss)\n",
    "        \n",
    "    #mean_val_losses.append(np.mean(val_losses))    \n",
    "    time_elapsed = time.time() - since\n",
    "    val_epochloss = np.mean(val_losses) #/num_batches\n",
    "    \n",
    "    if val_epochloss < val_currepochloss:\n",
    "        print('save the weights')\n",
    "        torch.save(net.state_dict(),\"./UNET_weight_DM/UNET_segmentation_DM_epochs_new_val_check.pth\")\n",
    "        flag = 0\n",
    "        val_currepochloss = val_epochloss\n",
    "    else:\n",
    "        flag += 1\n",
    "        \n",
    "        if flag == 6:\n",
    "            for p in optimizer.param_groups:\n",
    "                lr2 = p['lr']\n",
    "            newlr = lr2/5\n",
    "            \n",
    "            if newlr < 1e-15:\n",
    "                print(\"Cant decrease further!!\")\n",
    "                newlr = 1e-15\n",
    "            flag = 0 \n",
    "            optimizer = optim.SGD(net.parameters(), lr=newlr, momentum=0.9)\n",
    "            print(\"Learning rate changed from \"+str(lr2)+\" to \"+str(newlr))\n",
    "            \n",
    "        print(\"Loss \"+str(val_epochloss)+\" is bigger than Loss \"+str(val_currepochloss)+\" in the prev epoch \")\n",
    "        \n",
    "    print('Loss = {:.8f} val Loss = {:.8f} at epoch {:d}  completed in {:.0f}m {:.0f}s'.format(epochloss,np.mean(val_losses),(i+1),(time_elapsed//60),(time_elapsed%60)))\n",
    "    i += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest3[0]=xtrainT[1]\n",
    "print(xtest3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for params in optimizer.param_groups:\n",
    "    print(params['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.cuda()\n",
    "net.load_state_dict(torch.load(\"./UNET_weight_DM/UNET_segmentation_DM_epochs_new_407.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = net.cuda()\n",
    "#testing of the architecture...\n",
    "num_batches = 0\n",
    "#6 evenly divides the test batch size..\n",
    "test_batch_size = 2\n",
    "n_examples = 6\n",
    "#finalpred = Variable(torch.zeros((n_examples,3,120,160)))\n",
    "finalpred = Variable(torch.zeros((n_examples,512,512)))\n",
    "print(\"finalpred size is ---> \", finalpred.size())\n",
    "\n",
    "num_batches = n_examples//test_batch_size\n",
    "print(\"num of batches --->\", num_batches)\n",
    "i=10\n",
    "for k in range(num_batches):\n",
    "    start, end = (i)*test_batch_size, (i+1)*test_batch_size\n",
    "    start1, end1 = (k)*test_batch_size, (k+1)*test_batch_size\n",
    "    output = net.forward(Variable(xtestT[start1:end1], volatile=True).cuda())\n",
    "    finalpred[start1:end1] = output\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.rand(2,2)\n",
    "\n",
    "p = d.numpy()\n",
    "\n",
    "#Variables are wrappers around tensors that save operation history. the data can accessed with below method\n",
    "data = finalpred.data.numpy()\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xtestT.size()[0])\n",
    "\n",
    "ind = np.random.permutation(xtestT.size()[0])\n",
    "print(ind[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['image.interpolation'] = 'none'\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(9)\n",
    "fig.set_figwidth(9)\n",
    "ind = 0\n",
    "flag = 0\n",
    "ind11 = np.random.permutation(xtestT.size()[0])\n",
    "imageinds = [ind11[0],ind11[1],ind11[2],ind11[3],ind11[4]]\n",
    "#imageinds = [200,111,34,29,32]\n",
    "\n",
    "#imageinds = [463,464,465,466,467]\n",
    "#imageinds = [155,156,157,158,159]\n",
    "\n",
    "for i in range(1,21,4):\n",
    "    nind = imageinds[ind]\n",
    "    \n",
    "    img1 = np.zeros((512,512,3))\n",
    "    temp = xtestT[nind]\n",
    "    #print(finxtest1[5].shape)\n",
    "\n",
    "    img1[:,:,0] = temp[0,:,:]\n",
    "    img1[:,:,1] = temp[1,:,:]\n",
    "    img1[:,:,2] = temp[2,:,:]\n",
    "    img1 = imresize(img1,(512,512,3))\n",
    "    \n",
    "    a=fig.add_subplot(5,4,i)\n",
    "    imgplot = plt.imshow(img1)\n",
    "    if flag == 0:\n",
    "        a.set_title('Input')\n",
    "    a.axes.get_xaxis().set_visible(False)\n",
    "    a.axes.get_yaxis().set_visible(False)\n",
    "    a=fig.add_subplot(5,4,i+1)\n",
    "    ##################################################\n",
    "    testPT = xtestT[nind]\n",
    "    testPT = testPT.view(1,3,512,512)\n",
    "    test_pred = net.forward(Variable(testPT, volatile=True).cuda())\n",
    "    testx11 = test_pred.cpu()\n",
    "    testx = testx11.detach().numpy()\n",
    "    testx = np.reshape(testx,(1,512,512))\n",
    "    testx = testx.transpose(1,2,0)\n",
    "    testx = cv2.resize(testx,(512,512))\n",
    "    imgplot = plt.imshow(testx)\n",
    "    imgplot.set_cmap('jet')\n",
    "    #print(\"hi inside\")\n",
    "    if flag == 0:\n",
    "        a.set_title('Predicted Depth')\n",
    "    a.axes.get_xaxis().set_visible(False)\n",
    "    a.axes.get_yaxis().set_visible(False)\n",
    "    ##################################################\n",
    "    a=fig.add_subplot(5,4,i+2)\n",
    "    imgplot = plt.imshow(ytestT[nind])\n",
    "    imgplot.set_cmap('jet')\n",
    "    if flag == 0:\n",
    "        a.set_title('Actual Depth')\n",
    "    a.axes.get_xaxis().set_visible(False)\n",
    "    a.axes.get_yaxis().set_visible(False)\n",
    "    ##################################################\n",
    "    a=fig.add_subplot(5,4,i+3)\n",
    "    ytest = ytestT[nind].numpy()\n",
    "    error1 = np.abs(testx-ytest)\n",
    "    #print \"max is --->\", error1.max()\n",
    "    #print \"min is --->\", error1.min()\n",
    "    imgplot = plt.imshow(error1)\n",
    "    if flag == 0:\n",
    "        a.set_title('Absolute Error')\n",
    "    #imgplot.set_rotation(0)\n",
    "    a.axes.get_xaxis().set_visible(False)\n",
    "    a.axes.get_yaxis().set_visible(False)\n",
    "    #imgplot.set_cmap('hot')\n",
    "    imgplot.set_cmap('jet')\n",
    "    plt.colorbar(fraction=0.080, pad=0.03)\n",
    "    #plt.colorbar(fraction=0.080, pad=0.03,ticks=[0.0,0.2,0.4,0.6])\n",
    "    #plt.clim(0,1)\n",
    "    plt.clim(vmin=0, vmax=0.7)\n",
    "    ##################################################\n",
    "    ind += 1\n",
    "    flag = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['image.interpolation'] = 'none'\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(9)\n",
    "fig.set_figwidth(9)\n",
    "import cv2\n",
    "\n",
    "ind = 200\n",
    "testPT = xtestT[ind]\n",
    "\n",
    "testPT = testPT.view(1,3,512,512)\n",
    "test_pred = net.forward(Variable(testPT, volatile=True).cuda())\n",
    "testx = testPT.numpy()\n",
    "testx = np.reshape(testx,(3,512,512))\n",
    "testx = testx.transpose(1,2,0)\n",
    "testx = imresize(testx,(512,512,3))\n",
    "#imshow(testx)\n",
    "scipy.misc.imsave('test_200.png', testx)\n",
    "a=fig.add_subplot(1,2,1)\n",
    "imgplot = plt.imshow(testx)\n",
    "a.set_title('Input')\n",
    "a.axes.get_xaxis().set_visible(False)\n",
    "a.axes.get_yaxis().set_visible(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "tar = ytestT[ind]\n",
    "print(tar.shape)\n",
    "\n",
    "\n",
    "tar_x = tar.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#testx = testPT.numpy()\n",
    "tar_x = np.reshape(tar_x,(1,512,512))\n",
    "print(testx.shape)\n",
    "tar_x = tar_x.transpose(1,2,0)\n",
    "print(tar_x.shape)\n",
    "tar_x = cv2.resize(tar_x,(512,512))\n",
    "scipy.misc.imsave('tar_200.png', tar_x)\n",
    "imgplot = plt.imshow(tar_x)\n",
    "imgplot.set_cmap('jet')\n",
    "print(tar_x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(test_pred.shape)\n",
    "\n",
    "\n",
    "testx11 = test_pred.cpu()\n",
    "print(testx11.shape)\n",
    "testx = testx11.detach().numpy()\n",
    "print(testx.shape)\n",
    "testx = np.reshape(testx,(1,512,512))\n",
    "print(testx.shape)\n",
    "testx = testx.transpose(1,2,0)\n",
    "print(testx.shape)\n",
    "testx = cv2.resize(testx,(512,512))\n",
    "scipy.misc.imsave('pred_200.png', testx)\n",
    "imgplot = plt.imshow(testx)\n",
    "imgplot.set_cmap('jet')\n",
    "print(testx.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error=abs(tar_x-testx)\n",
    "print(error.shape)\n",
    "scipy.misc.imsave('error_200.png', error)\n",
    "imgplot = plt.imshow(error)\n",
    "imgplot.set_cmap('jet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootDir1 = './results_DM/img'\n",
    "rootDir2 = './results_DM/pred'\n",
    "rootDir3 = './results_DM/tar'\n",
    "rootDir4 = './results_DM/error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xtestT.size()[0])\n",
    "d=0\n",
    "for i in range(xtestT.size()[0]):\n",
    "    #print(xtestT[i].size())\n",
    "    img = xtestT[i]\n",
    "    img = img.view(1,3,512,512)\n",
    "    test_pred = net.forward(Variable(img, volatile=True).cuda())\n",
    "    img_x = img.numpy()\n",
    "    img_x = np.reshape(img_x,(3,512,512))\n",
    "    img_x = img_x.transpose(1,2,0)\n",
    "    img_x = imresize(img_x,(512,512,3))\n",
    "    fname1=\"%d.tif\"%d\n",
    "    cv2.imwrite(os.path.join(rootDir1,fname1),img_x)\n",
    "    ############################################################\n",
    "    testx11 = test_pred.cpu()\n",
    "    testx = testx11.detach().numpy()\n",
    "    testx = np.reshape(testx,(1,512,512))\n",
    "    testx = testx.transpose(1,2,0)\n",
    "    testx = cv2.resize(testx,(512,512))\n",
    "    scipy.misc.imsave(os.path.join(rootDir2,fname1), testx)\n",
    "    #############################################################\n",
    "    tar = ytestT[i]\n",
    "    tar_x = tar.numpy()\n",
    "    tar_x = np.reshape(tar_x,(1,512,512))\n",
    "    tar_x = tar_x.transpose(1,2,0)\n",
    "    tar_x = cv2.resize(tar_x,(512,512))\n",
    "    scipy.misc.imsave(os.path.join(rootDir3,fname1), tar_x)\n",
    "    ###############################################################\n",
    "    error=abs(tar_x-testx)\n",
    "    scipy.misc.imsave(os.path.join(rootDir4,fname1), error)\n",
    "    ##############################################################\n",
    "    d+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
